## Ziel des Tools

LLMEval ist eine Anwendung zum Evaluieren und Verbessern der Qualität der Antworten von Large Language Models (LLMs) unter Verwendung von Metriken, Frage-Antwort-Katalogen und konfigurierten Endpunkten. Das Ziel ist, die Zuverlässigkeit und Genauigkeit von GenAI Produkten sicherzustellen.

## Workflow (typischer Ablauf):

1. **LLM Endpoint Konfigurieren**: Gehe zum LLM Endpoints Bereich. Definiere Name, Pfad zum Server, Key und das zu verwendende Model. Konfiguriere die Parameter für Anfrage und das zu verwendende Model.
2. **QA Katalog erstellen**: Erstelle einen QA Katalog. Ein Katalog kann entweder hochgeladen werden oder synthetisch generiert werden.
3. **Metriken konfigurieren**: Wähle Metriken aus, die für die Qualität der Antworten relevant sind und passen sie an. Definiere einen Schwellwert ab wann die Qualität nicht mehr ausreichend ist.
4. **Auswertung starten**: Starte die Auswertung. Wähle den zu verwendenden QA-Katalog, die Metriken und den zu verwendenden LLM Endpunkt.
5. **Ergebnisse analysieren**: Analysiere die Ergebnisse. Wurden Schwellwerte eingehalten? Welche Bereiche müssen verbessert werden?
6. **LLM Verbessern und Wiederholen**: Passe deine LLM-Modelle oder Prompts an und wiederhole den Evaluationsprozess, um die Ergebnisse zu verfeinern.
