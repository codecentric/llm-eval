## Goal of the Tool

LLMEval is an application for evaluating and improving the quality of Large Language Model (LLM) responses using metrics for question-answer catalogs on configured endpoints. The goal is to ensure the reliability and accuracy of generative AI products.

## Workflow (typical process):

1.  **Configure LLM Endpoint**: Go to the LLM Endpoints section. Define Name, Path to the server, Key, and the model to use.
2.  **Create QA Catalog**: Create a QA catalog. A catalog can be either uploaded or generated synthetically.
3.  **Configure Metrics**: Select metrics relevant to response quality and customize them. Define a threshold below which the quality is no longer sufficient.
4.  **Start Evaluation**: Start the evaluation. Select the QA catalog to use, the metrics, and the LLM endpoint to use.
5.  **Analyze Results**: Analyze the results. Were thresholds met? What areas need improvement?
6.  **Improve LLM and Repeat**: Adjust your LLM models or prompts and repeat the evaluation process to refine the results.
