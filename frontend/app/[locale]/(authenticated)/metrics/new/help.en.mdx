## Select Type

Here you select the type of metric. Existing options are:

- G-Eval
- Answer Relevancy
- Credibility
- Hallucination

## Configuration

This area changes dynamically depending on which metric type was selected in step 1. Contains the following fields:

- **Name**: A user-defined name for the metric. Used for identification in evaluations.
- **Threshold**: A numeric value that serves as the minimum value (or maximum value, depending on the metric) for meeting the quality criterion. The threshold must be chosen relative to the value range of the respective metric. For "Answer Relevancy," a value of 0.5 is likely sensible, as the metric probably returns values between 0 and 1.
- **Strict Mode**: A binary value (Yes/No) that affects the behavior of the metric. If set to "Yes," stricter criteria are applied during the evaluation. The exact meaning depends on the chosen metric type.
- **Evaluation Steps**: (Only for G-Eval) A list of statements that influence the criteria for evaluating the LLM response. The selection determines which aspects of the response are prioritized (such as factuality, level of detail, and avoidance of vague language). Adding or removing individual steps can influence the evaluation of the results.

### Evaluation Parameter

- **Chat Model Endpoint**: Selection of a configured LLM endpoint to be used for evaluation. A suitable endpoint must be selected here, which is suitable for the desired metric.
- **Threshold**
- **Specify reason**
- **Strict mode**

(Only with G-Eval) Assignment of the parameters to be used for the evaluation. The following parameters are available:

- **Input**: The input given to the LLM.
- **Actual Output**: Output generated by the LLM.
- **Expected Output**: The expected output of the LLM
- **Context**: Additional context to be included in the evaluation.
- **Query context**: describes the context of the query
- **Expected Tools**
- **Called tools**
- **Reasoning**

## Explanation of Metric Types

### G-Eval (GPT-based Evaluation)

Uses another LLM (e.g., GPT-4) to evaluate the quality of the primary LLM's response. The "evaluation LLM" receives the question, the primary LLM's response, and, if applicable, a reference answer. It then provides a scoring that reflects the quality of the primary response. This metric assesses the overall quality of the response. This includes aspects such as relevance, coherence, accuracy, completeness, and style. G-Eval is flexible and can be adapted to various domains and requirements. The adjustment is made via the evaluation steps.

### Answer Relevancy

Measures how well the LLM's response answers the question asked. Algorithms such as text similarity scores (e.g., cosine similarity of embeddings) or pre-trained models for assessing relevance can be used. This metric ensures that the LLM does not answer off-topic or provide irrelevant information.

### Credibility

Evaluates how credible the LLM response is. Can be based on external knowledge sources (e.g., knowledge databases, APIs) to verify the facts in the response. This metric assesses whether the LLM response is trustworthy, well-founded, and of high quality. Avoids responses that include speculations or opinions.

### Hallucination

Measures whether the LLM invents facts or includes inaccurate information in its response. Methods for detecting hallucinations can include fact-checking APIs or comparison with reference texts. This metric prevents the LLM from generating false or misleading information. Particularly important in applications where accuracy is crucial.

> Important: The definition of the metrics depends on the respective use case. LLMEval only offers the possibility to evaluate the contents generated by LLMs based on these metrics.
